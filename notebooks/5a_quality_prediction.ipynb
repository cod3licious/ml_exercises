{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Case Study: Cement Compressive Strength Prediction\n",
    "\n",
    "In this case study, you'll solve a machine learning problem from start to finish.\n",
    "\n",
    "The previous notebook, \"analyze toydata\", deals with a similar problem and can serve as a guideline for this exercise. You may also want to have a look at the [cheat sheet](https://franziskahorn.de/mlws_resources/cheatsheet.pdf) for more ideas and a concise overview of the relevant steps when developing a machine learning solution in any data science project. \n",
    "\n",
    "Feel free to get creative! \n",
    "\n",
    "### The Task\n",
    "\n",
    "You are a data scientist at a startup that wants to build an app to help cement plants produce cement with a certain target compressive strength (= the main quality parameter of cement). Producing cement with a given strength is difficult, because the real compressive strength of a cement sample can only be measured after 28 days, at which point it is far too late to change anything in the production process and the produced cement needs to potentially be discarded.\n",
    "\n",
    "During the cement production, some measurements about the material composition are available as well as the fineness of the cement powder. If you manage to predict the cement strength correctly from these parameters, then the cement production can be adjusted by changing the settings of the cement mill to make the cement powder finer or coarser. The cement plant already knows that milling the cement 1 micron finer increases the compressive strength by about 1 MPa, however, without accurate predictions of the current strength, they don't know when to change the fineness and therefore always mill each cement type with a fixed fineness that worked well on average in the past.\n",
    "\n",
    "Your colleagues have already prepared the backend for the app (see notebook `b`), which will load your model to make the predictions. Additionally, the backend also contains code to optimize the cement's fineness to get closer to the specified target strength.\n",
    "\n",
    "**Your job is it to build an ML model that predicts a cement sample's compressive strength.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import requests\n",
    "import joblib\n",
    "from sklearn.dummy import DummyRegressor\n",
    "from sklearn.model_selection import TimeSeriesSplit, cross_val_score\n",
    "\n",
    "# these \"magic commands\" are helpful if you plan to import functions from another script\n",
    "# where you keep changing things, i.e., if you change a function in the script\n",
    "# it will automagically be reloaded in the notebook so you work with the latest version\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 0: Load the data\n",
    "\n",
    "The data contains samples from 5 different cement types, collected every Monday, Wednesday, and Friday. The data includes the compressive strength after 28 days as well as the fineness and various measurements of the material composition. Our goal is to **predict the compressive strength** of a cement sample based on the other values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the file into a dataframe with pandas\n",
    "df = pd.read_csv(\"../data/cement_samples.csv\")\n",
    "# transform the timestamp into proper datetimes (for plotting)\n",
    "df[\"timestamp\"] = pd.to_datetime(df[\"timestamp\"])\n",
    "df = df.set_index(\"timestamp\").sort_index()\n",
    "# look at the raw data (first 5 rows)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define features (all measurements known initially) and target (strength known only after 28 days)\n",
    "# CAUTION: if you change these, you also need to adapt the feature list in the backend!\n",
    "FEATURES = [\"fineness\"] + [f\"material_{i}\" for i in range(1, 16)]\n",
    "TARGET = \"strength\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# these are the target strengths for each cement type\n",
    "target_strengths = {\n",
    "    \"CEM I 52\": 59.5,\n",
    "    \"CEM I 42\": 50.5,\n",
    "    \"CEM II AS 52\": 57.5,\n",
    "    \"CEM II AS 42\": 50.,\n",
    "    \"CEM II BLL 32\": 40.,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Exploratory Analysis\n",
    "\n",
    "Get a better understanding of the data and the problem:\n",
    "- How are the individual variables distributed?\n",
    "- What differences can you observe between the different cement types?\n",
    "- Does the data contain outliers?\n",
    "- Can you observe any drifts in the values over time?\n",
    "- Are any variables correlated (especially with the target)?\n",
    "- Anything else you should take into account when preprocessing the data later for the supervised learning task?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: create some plots to better understand the data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Predict the 28-day Compressive Strength\n",
    "\n",
    "Now that you've become more familiar with the dataset, it's time to tackle the real task, i.e., predict the 28-day compressive strength of a cement sample.\n",
    "\n",
    "An evaluation pipeline is already set up below using a \"stupid baseline\" (= predicting the mean). Your task is to improve upon the performance by trying, for exampe... \n",
    "- different [models](https://scikit-learn.org/stable/supervised_learning.html)\n",
    "- different [preprocessing steps](https://scikit-learn.org/stable/modules/preprocessing.html) (e.g., transformations or feature engineering)\n",
    "- [hyperparameter tuning](https://scikit-learn.org/stable/modules/grid_search.html)\n",
    "- [ensemble models](https://scikit-learn.org/stable/modules/ensemble.html) (i.e., combining different models)\n",
    "\n",
    "Get creative :-)\n",
    "\n",
    "**Tip:** To use your model within the app's backend later (notebook `b`), it's important that your final model incl. all necessary preprocessing steps are combined in a single estimator object. This can be accomplished with sklearn's [`make_pipeline`](https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.make_pipeline.html) function. If necessary, you could even write a [custom transformer](https://towardsdatascience.com/pipelines-custom-transformers-in-scikit-learn-ef792bbb3260/) to perform more fancy feature engineering steps than what is provided by sklearn.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract a training and final test set from the full dataframe\n",
    "# since the samples are collected over time, we use the most recent data for testing\n",
    "# we also leave a 28 day gap between train and test to avoid data leakage\n",
    "df_train, df_test = df[df.index < \"2024-12-03\"], df[df.index >= \"2025-01-01\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_model(model, df_train):\n",
    "    \"\"\"\n",
    "    Function to evaluate a regressor using a timeseries cross-validation.\n",
    "    Prints mean absolute error, averaged across xval folds.\n",
    "    \n",
    "    The best possible MAE (if you had access to some hidden information) is 0.3 (due to measurement noise).\n",
    "    A realistic good MAE that you should try to achieve is 0.52\n",
    "    \n",
    "    Inputs:\n",
    "        - model: the initialized model\n",
    "        - df_train: the training data\n",
    "    \"\"\"\n",
    "    # to make it realistic, we need to leave a 28 day gap - that is 3 samples * 4 weeks * number of cements\n",
    "    gap = 12 * df_train[\"cement\"].nunique()\n",
    "    tscv = TimeSeriesSplit(gap=gap, n_splits=10)\n",
    "    res = cross_val_score(model, df_train[FEATURES], df_train[TARGET], scoring=\"neg_mean_absolute_error\", cv=tscv)\n",
    "    print(f\"Cross-validated MAE: {np.mean(np.abs(res)):.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize a dummy model\n",
    "model = DummyRegressor()\n",
    "# evaluate the model\n",
    "eval_model(model, df_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: now it's up to you: try an actual model and get better predictions!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train your final model on the full training dataset\n",
    "model = model.fit(df_train[FEATURES], df_train[TARGET])\n",
    "# and save it so it can be used by the backend\n",
    "# CAUTION: your model must be a single estimator object that also includes all necessary preprocessing steps \n",
    "# (e.g., by using the `make_pipeline` function mentioned above) which works on the originally defined features\n",
    "joblib.dump(model, \"strength_model.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Interpret the model\n",
    "\n",
    "After you've found a model that achieves a decent performance, try to understand what it's doing.\n",
    "- Calculate the permutation feature importance to see which features are most influential overall\n",
    "- For the most important features, look at the partial dependence plot to see _how_ these features influence the outcome\n",
    "\n",
    "Do these results make sense given what you know about the data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: permutation feature importance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: partial dependence plots\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Use the backend\n",
    "\n",
    "Head over to notebook `b` and run it from top to bottom and leave it open. It runs the server with our backend using your trained and saved model to make predictions.\n",
    "\n",
    "In the code below we query the backend with a single data point to see what it returns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# url where the backend is running\n",
    "port = 8000\n",
    "backend_url = f\"http://localhost:{port}/predict\"\n",
    "# columns that the backend expects\n",
    "JSON_COLS = [\"cement\"] + FEATURES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get one row from our dataframe and convert it to a dictionary\n",
    "x = df_test[JSON_COLS].iloc[0].to_dict()\n",
    "# add the target strength\n",
    "x[\"target_strength\"] = target_strengths[x[\"cement\"]]\n",
    "# send this as a json to the backend via a POST request\n",
    "response = requests.post(backend_url, json=x)\n",
    "# the result will also be dictionary with\n",
    "# - the original fineness\n",
    "# - the predicted strength with the original fineness\n",
    "# - the optimized fineness\n",
    "# - the new predicted strength when using the optimized fineness\n",
    "result = response.json()\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Optimization & What-If Analysis\n",
    "\n",
    "When producing cement, the quality of the raw materials is fixed (whatever we dig up in the quarry) but the fineness can be adjusted in the mill to compensate for quality differences and change the resulting compressive strength of the cement in accordance with this cement's target strength.\n",
    "\n",
    "Your prediction model is used in the backend to check whether the cement would get too strong or too weak and then the fineness is adapted accordingly to get closer to the target.\n",
    "\n",
    "Does your model help to get the production on target?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_what_if(df_cem, target_strength):\n",
    "    \"\"\"\n",
    "    Compute and plot the optimized the fineness for all samples from one cement.\n",
    "    This code creates two plots:\n",
    "        1. What-if results after optimization: by changing the fineness, the strength predictions\n",
    "           should be closer to the target strength, even after correcting for prediction errors.\n",
    "           The legend includes the MATD = mean absolute target deviation, i.e., how far away the\n",
    "           respective points are from the target strength on average.\n",
    "        2. Original and optimized fineness and resulting strength increase/decrease.\n",
    "    \n",
    "    Inputs:\n",
    "        - df_cem: pandas dataframe with the data from one cement\n",
    "        - target_strength: what we would like the compressive strength to be\n",
    "    \"\"\"\n",
    "    # copy data so we don't change the original\n",
    "    df = df_cem.copy()\n",
    "    # run the optimization for all data points\n",
    "    fineness_org_s, fineness_new_s, pred_org_s, pred_new_s = [], [], [], []\n",
    "    for i in range(len(df)):\n",
    "        if not i % 10:\n",
    "            print(f\"backend queried for {i:2} / {len(df)} data points\", end=\"\\r\")\n",
    "        x = df[JSON_COLS].iloc[i].to_dict()\n",
    "        x[\"target_strength\"] = target_strength\n",
    "        result = requests.post(backend_url, json=x).json()\n",
    "        fineness_org_s.append(result[\"fineness_org\"])\n",
    "        fineness_new_s.append(result[\"fineness_new\"])\n",
    "        pred_org_s.append(result[\"pred_org\"])\n",
    "        pred_new_s.append(result[\"pred_new\"])\n",
    "    print(f\"backend queried for {len(df)} / {len(df)} data points\")\n",
    "    # add results to dataframe for easier plotting\n",
    "    df[\"fineness_org\"] = fineness_org_s\n",
    "    df[\"fineness_new\"] = fineness_new_s\n",
    "    df[\"pred_org\"] = pred_org_s\n",
    "    df[\"pred_new\"] = pred_new_s\n",
    "    \n",
    "    # plot the optimization results\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    # dashed line to indicated the target value we wanted\n",
    "    plt.hlines(target_strength, df.index.min(), df.index.max(), \"k\", \"dashed\", linewidth=1)\n",
    "    # original strength values as light blue dots\n",
    "    plt.plot(df[TARGET], \"o\", c=\"#53DDFE\", alpha=0.8, \n",
    "             label=f\"original y (MATD: {np.abs(df[TARGET] - target_strength).mean():.1f})\")\n",
    "    # predicted strength values with original fineness as blue x\n",
    "    plt.plot(df[\"pred_org\"], \"x\", c=\"#007693\", \n",
    "             label=f\"predicted y (MATD: {np.abs(df[\"pred_org\"] - target_strength).mean():.1f})\")\n",
    "    # predicted strength values with optimized fineness as orange x\n",
    "    plt.plot(df[\"pred_new\"], \"x\", c=\"#A84801\", \n",
    "             label=f\"optimized predicted y (MATD: {np.abs(df[\"pred_new\"] - target_strength).mean():.1f})\")\n",
    "    # since our original predictions are not perfect, we shift our optimized predictions\n",
    "    # by the error we made on the original predictions -> plot as orange dots\n",
    "    pred_new_corrected_s = df[\"pred_new\"] + (df[TARGET] - df[\"pred_org\"])\n",
    "    plt.plot(pred_new_corrected_s, \"o\", alpha=0.8, c=\"#FE9C53\", \n",
    "             label=f\"realistic optimized y (MATD: {np.abs(pred_new_corrected_s - target_strength).mean():.1f})\")\n",
    "    plt.xlabel(\"samples\")\n",
    "    plt.ylabel(\"compressive strength [MPa]\")\n",
    "    plt.title(f\"What-If Analysis for {df['cement'].iloc[0]}\")\n",
    "    plt.legend(loc=2, bbox_to_anchor=(1.02, 1), numpoints=1)\n",
    "    \n",
    "    # plot original and optimized fineness\n",
    "    plt.figure()\n",
    "    # diagonal line -> original and optimized fineness are the same\n",
    "    min_max = [df[\"fineness_new\"].min(), df[\"fineness_new\"].max()]\n",
    "    plt.plot(min_max, min_max, \"k\", alpha=0.5)\n",
    "    # points above the line: finer than before\n",
    "    # points below the line: coarser than before\n",
    "    # color of dot shows whether the optimization resulted in a reduction or increase in predicted strength\n",
    "    plt.scatter(df[\"fineness_org\"], df[\"fineness_new\"], c=df[\"pred_new\"]-df[\"pred_org\"])\n",
    "    plt.colorbar()\n",
    "    plt.xlabel(\"original fineness\")\n",
    "    plt.ylabel(\"optimized fineness\")\n",
    "    plt.title(\"changes in fineness & strength\");\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run the optimization on the test set and plot the results for one cement\n",
    "cement = \"CEM II AS 42\"\n",
    "plot_what_if(df_test[df_test[\"cement\"] == cement], target_strengths[cement])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Presentation of results\n",
    "Clean up your code & think about which results you want to present and the story they tell:\n",
    "- What have you learned about cement and how is this reflected in the data?\n",
    "- What is the best model that you found & its performance?\n",
    "- Which preprocessing steps had the most impact on the performance (for different models)?\n",
    "- Which features were the most influential and how did they impact the model's prediction?\n",
    "- What have you learned in this case study? Did any of the results surprise you?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
